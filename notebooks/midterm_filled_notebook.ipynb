{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61f3650",
   "metadata": {},
   "source": [
    "# Applied AI Midterm — SRGAN + Classifiers (PyTorch)\n",
    "Complete runnable notebook template for the midterm. Edit dataset paths and hyperparameters where marked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165066f",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Edit DATA_ROOT and run cells. Install packages if needed: `!pip install timm torchmetrics` in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292343fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Imports and Globals\n",
    "# TODO: set DATA_ROOT to your dataset\n",
    "DATA_ROOT = 'C:/Users/lolze/Documents/Github/Midterm_AppliedAI/data/raw'  # <-- change me\n",
    "PROCESSED_128 = 'C:/Users/lolze/Documents/Github/Midterm_AppliedAI/data/processed_128'\n",
    "LOWRES_32 = 'C:/Users/lolze/Documents/Github/Midterm_AppliedAI/data/lowres_32'\n",
    "SEED = 42\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# TODO: run rest of the notebook cells to execute the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ae252",
   "metadata": {},
   "source": [
    "## 1. Prepare datasets (resize & save)\n",
    "This cell will resize images to 128x128 and 32x32 and save to folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a70134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: prepare_datasets (run once)\n",
    "from PIL import Image\n",
    "import os, shutil\n",
    "def prepare_datasets(root, out_hr, out_lr, size_hr=128, size_lr=32, force=False):\n",
    "    if force:\n",
    "        shutil.rmtree(out_hr, ignore_errors=True)\n",
    "        shutil.rmtree(out_lr, ignore_errors=True)\n",
    "    os.makedirs(out_hr, exist_ok=True)\n",
    "    os.makedirs(out_lr, exist_ok=True)\n",
    "    classes = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
    "    classes.sort()\n",
    "    for cls in classes:\n",
    "        in_dir = os.path.join(root, cls)\n",
    "        out_hr_cls = os.path.join(out_hr, cls); os.makedirs(out_hr_cls, exist_ok=True)\n",
    "        out_lr_cls = os.path.join(out_lr, cls); os.makedirs(out_lr_cls, exist_ok=True)\n",
    "        for fname in os.listdir(in_dir):\n",
    "            if not fname.lower().endswith(('.png','.jpg','.jpeg')): continue\n",
    "            src = os.path.join(in_dir, fname)\n",
    "            img = Image.open(src).convert('RGB')\n",
    "            img.resize((size_hr,size_hr), Image.BICUBIC).save(os.path.join(out_hr_cls, fname))\n",
    "            img.resize((size_lr,size_lr), Image.BICUBIC).save(os.path.join(out_lr_cls, fname))\n",
    "prepare_datasets(DATA_ROOT, PROCESSED_128, LOWRES_32, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b6a75",
   "metadata": {},
   "source": [
    "## 2. Dataloaders\n",
    "Create ImageFolder datasets and DataLoaders for HR and LR images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad125aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR samples 25000 LR samples 25000\n"
     ]
    }
   ],
   "source": [
    "# Cell: dataloaders\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "mean = [0.485,0.456,0.406]; std=[0.229,0.224,0.225]\n",
    "transform_hr = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "transform_lr = transforms.Compose([transforms.ToTensor()])\n",
    "hr_dataset = datasets.ImageFolder(PROCESSED_128, transform=transform_hr)\n",
    "lr_dataset = datasets.ImageFolder(LOWRES_32, transform=transform_lr)\n",
    "print('HR samples', len(hr_dataset), 'LR samples', len(lr_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15312eec",
   "metadata": {},
   "source": [
    "## 3. Classifier A — Transfer learning\n",
    "This section contains a full training loop using timm EfficientNet-B0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574d659c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "File \u001b[1;32mc:\\Users\\lolze\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_vendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[0;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "# Cell: classifier A - Transfer learning (Complete Implementation)\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "modelA = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2)\n",
    "modelA = modelA.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "dataset_size = len(hr_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(hr_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelA.parameters(), lr=0.0001)\n",
    "\n",
    "# Setup directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "init_time = datetime.now()\n",
    "current_time = init_time.strftime('%Y%m%d_%H%M%S')\n",
    "name_dir = f'models/classifier_A_{current_time}'\n",
    "os.makedirs(name_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=os.path.join(name_dir, 'logs'))\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    modelA.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelA(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    modelA.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = modelA(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy()[:, 1])  # Probability for class 1\n",
    "    \n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    val_f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    val_auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "    print(f'  Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Tensorboard logging\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/val', val_f1, epoch)\n",
    "    writer.add_scalar('AUC/val', val_auc, epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(modelA.state_dict(), os.path.join(name_dir, 'classifier_A_best.pth'))\n",
    "        print(f'  -> New best model saved! Accuracy: {val_accuracy:.2f}%')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered!')\n",
    "            break\n",
    "\n",
    "writer.close()\n",
    "print(f\"Training completed. Best validation accuracy: {best_val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278c13b",
   "metadata": {},
   "source": [
    "## 4. SRGAN (Generator + Discriminator)\n",
    "Contains model definitions and training loop skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78240774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: SRGAN models skeleton\n",
    "import torch.nn as nn\n",
    "# TODO: paste Generator and Discriminator classes (e.g., SRResNet-like) and training loop\n",
    "# Save checkpoints every n epochs to models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38b77c",
   "metadata": {},
   "source": [
    "## 5. Generate synthetic images\n",
    "Use trained generator to create HR images from LR inputs and save to data/generated_128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: generation script\n",
    "# TODO: load generator checkpoint and run generation loop saving outputs per class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82bbaf",
   "metadata": {},
   "source": [
    "## 6. Classifier B (train with generated data)\n",
    "Train classifier on combined real + generated dataset. Save models/classifier_B_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7377a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: classifier B skeleton\n",
    "# TODO: create combined dataset by copying generated images into a copy of processed_128, then train using same recipe as classifier A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42098a",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "Compute Accuracy, F1, AUC, show confusion matrices, ROC curves, and sample images (LR / Bicubic / GAN / GT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: evaluation\n",
    "# TODO: implement evaluation code using sklearn.metrics and plotting utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c000c8",
   "metadata": {},
   "source": [
    "## 8. Tips & Next steps\n",
    "- Set EPOCHS_SR = 150 for final training.\n",
    "- Use mixed precision (torch.cuda.amp) to speed up.\n",
    "- Save checkpoints and push to Google Drive if using Colab.\n",
    "\n",
    "Good luck! If you'd like, I can now fill in the full detailed code for the SRGAN generator/discriminator and the training loops directly in this notebook (long cells). Reply 'yes' to have me expand the SRGAN training code inline in the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
